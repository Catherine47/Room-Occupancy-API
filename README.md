# Room-Occupancy-API
Optimizing Database Performance with PostgreSQL and Node.js
1. Introduction
The project focuses on optimizing database performance, an essential requirement in handling the large volumes of data generated by IoT devices. Sensor data related to room occupancy, obtained from the UC Irvine Machine Learning Repository, forms the basis of this initiative. The primary goal is to design a system capable of fast and efficient querying and data manipulation, providing scalability for future growth.
The methodology includes leveraging PostgreSQL to create an optimized database schema, complemented by Node.js and Express.js for backend API development. Additionally, the project integrates Swagger to simplify API documentation and testing. By addressing key challenges such as efficient storage and querying, the project showcases a practical solution to managing IoT-generated sensor data while maintaining performance and scalability.
2. Project Overview
The system designed in this project serves as a room occupancy management solution that utilizes PostgreSQL to store sensor readings and Node.js for backend processing. The sensor readings are organized using a well-thought-out database schema to facilitate efficient data storage, retrieval, and manipulation.
In addition to database optimization, the backend API provides CRUD operations, enabling users to interact seamlessly with the stored sensor data. Core features of the system include schema design with indexing, a RESTful API, and performance testing to ensure optimal functionality. Together, these components work to address challenges in scalability, efficient data handling, and user accessibility.
3. Database Design
The database design centers around a table named sensor_readings in the PostgreSQL database, called room_occupancy. This table is structured to store sensor data collected from various room sensors monitoring conditions such as temperature and light intensity. The schema includes columns for date, time, and sensor readings, each tailored to handle specific types of data, such as floating-point numbers for temperature and integers for light intensity.
To improve query performance, indexes have been created on the date and time columns, which are frequently used in filtering and sorting. This minimizes query execution time and enhances system scalability, particularly when handling large datasets. The design incorporates provisions for efficient storage and retrieval, ensuring that the database can support complex operations without degradation in performance.
4. API Development
The API development utilizes Node.js as the backend runtime and Express.js as the web framework to build RESTful endpoints. PostgreSQL serves as the relational database, managing sensor data efficiently, while Swagger is used for API documentation and interactive testing.
Several endpoints have been implemented to provide CRUD operations for the sensor readings. These include GET /sensor-readings, which retrieves all sensor readings with support for pagination and date filtering, and POST /sensor-readings, which adds new records to the database. Additionally, PUT /sensor-readings updates existing records, and DELETE /sensor-readings removes records based on specified criteria.
The API is designed to handle large volumes of data efficiently, using query parameters like date, page, and limit to refine searches and ensure scalability. With Swagger documentation, users can interact with the API via a user-friendly interface, accessible at http://localhost:3000/api-docs.
5. Performance Optimization
Optimizing database performance is a key focus of this project. Indexing has been employed to enhance query execution speed, especially for the date and time columns frequently used in filtering operations. SQL query optimization techniques have been applied to reduce computation time and improve data retrieval efficiency.
Stored procedures, such as get_sensor_readings_by_date, have been implemented to offload computational tasks from the backend, ensuring faster query processing. Pagination has been integrated into the API to limit the number of records returned per request, reducing the load on both the database and the client. Additionally, connection pooling techniques have been applied to enhance scalability and manage concurrent access to the database effectively.

6. Testing and Evaluation
Testing involved both manual and automated methods to ensure the reliability and functionality of the API. Manual testing using tools like Postman and curl commands verified endpoint functionality and parameter handling. Automated scripts were employed to test for edge cases, such as missing required fields and invalid data types.
Performance evaluation focused on assessing query execution under varying conditions, such as changing pagination limits and filters. Results demonstrated the system's ability to handle large datasets efficiently, delivering quick response times and reliable operations. These tests validated the project's success in addressing the challenges of scalability and efficient data management.
7. Conclusion
This project focuses on enhancing database performance by utilizing PostgreSQL for data management and Node.js for backend development. It addresses challenges commonly faced in managing large-scale data, such as efficient storage, quick retrieval, and scalability. Using sensor data from the UC Irvine Machine Learning Repository, the project demonstrates the application of advanced optimization techniques to ensure the system's effectiveness and reliability.
The initiative emphasizes creating a well-structured database schema and integrating a robust API through Node.js and Express.js to perform CRUD operations. Additionally, it incorporates Swagger to streamline API testing and documentation, making the system more accessible and user-friendly. By overcoming key data handling issues, the project provides a scalable solution well-suited for modern, data-intensive applications.
